{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from string import punctuation\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"./data_twitter/tweets_train.csv\",encoding='utf-8')\n",
    "train, validate, test = np.split(data.sample(frac=1), [int(.6*len(data)), int(.8*len(data))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(293159, 4)\n",
      "(97720, 4)\n",
      "(97720, 4)\n"
     ]
    }
   ],
   "source": [
    "print train.shape\n",
    "print validate.shape\n",
    "print test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embeddings={}\n",
    "with open('./glove.twitter.27B/glove.twitter.27B.200d.txt','r') as f:\n",
    "    for line in f:\n",
    "        list_line = line.split()\n",
    "        word= list_line[0]\n",
    "        word= word.replace('<',' ')\n",
    "        word = word.replace('>', ' ')\n",
    "        embeddings[word] = np.array(list_line[1:],dtype=np.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _tweets_preprocess(tweets):\n",
    "    tweets_clean=np.zeros_like(tweets)\n",
    "    for i,tweet in enumerate(tweets):\n",
    "        p = re.compile(r'(\\n)|(\\r)|(\\t)|(\\')|(\\u00A9)|([!\"#$%&()*+,-./:;<=>?@\\[\\\\\\]^_`{|}~])', re.IGNORECASE)\n",
    "        tweet = re.sub('\\?', ' ', tweet)\n",
    "        tweet = re.sub('\\.', ' ', tweet)\n",
    "        tweet = re.sub(',', ' ', tweet)\n",
    "        tweet = re.sub('!', ' ', tweet)\n",
    "        tweet = re.sub(' +',' ', tweet)\n",
    "        tweet = re.sub(p,\" \",tweet)\n",
    "        tweet= tweet.encode('ascii','ignore')\n",
    "\n",
    "        tweet_clean = [ wd.strip(punctuation).lower() for wd in tweet.split() \\\n",
    "                    if not wd.startswith('@') and not wd.startswith('#') and not wd == 'rt']\n",
    "        #wd[0].isupper()\n",
    "        \n",
    "        tweets_clean[i]= np.array(tweet_clean)\n",
    "    return tweets_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train= _tweets_preprocess(train['text'].values)\n",
    "y_train= train['label'].values\n",
    "X_val= _tweets_preprocess(validate['text'].values)\n",
    "y_val= validate['label'].values\n",
    "X_test= _tweets_preprocess(test['text'].values)\n",
    "y_test= test['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _calc_embeddings(X):\n",
    "    embeddings_data= np.zeros((X.shape[0],200))\n",
    "    for i,tweet in enumerate(X):\n",
    "        words=0\n",
    "        for token in tweet:\n",
    "            words+=1.0\n",
    "            try:\n",
    "                embeddings_data[i]+=embeddings[token.lower().strip()]\n",
    "            except:\n",
    "                continue\n",
    "        embeddings_data[i]/=words\n",
    "    return embeddings_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embeddings_train= _calc_embeddings(X_train)\n",
    "embeddings_val= _calc_embeddings(X_val)\n",
    "embeddings_test= _calc_embeddings(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder = MultiLabelBinarizer()\n",
    "encoder.fit(y_train[:,np.newaxis])\n",
    "y_train = encoder.transform(y_train[:,np.newaxis])\n",
    "y_val = encoder.transform(y_val[:,np.newaxis])\n",
    "y_test  = encoder.transform(y_test[:,np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(293159, 200)\n",
      "(293159, 20)\n"
     ]
    }
   ],
   "source": [
    "print embeddings_train.shape\n",
    "print y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras import backend as K\n",
    "from keras.callbacks import History "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f1(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "\n",
    "        Only computes a batch-wise average of recall.\n",
    "\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "\n",
    "        Only computes a batch-wise average of precision.\n",
    "\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createmodel():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(512,input_shape=(200,),activation='relu'))\n",
    "    model.add(Dense(256,activation='relu'))\n",
    "    model.add(Dense(128,activation='relu'))\n",
    "    model.add(Dense(64,activation='relu'))\n",
    "    model.add(Dense(32,activation='relu'))\n",
    "    #model.add(Dense(32,activation='relu'))\n",
    "    model.add(Dense(20,activation='softmax'))\n",
    "    model.summary()\n",
    "    model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=[f1,'accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_19 (Dense)             (None, 512)               102912    \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 20)                660       \n",
      "=================================================================\n",
      "Total params: 278,132\n",
      "Trainable params: 278,132\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 293159 samples, validate on 97720 samples\n",
      "Epoch 1/70\n",
      "293159/293159 [==============================] - 18s - loss: 2.3942 - f1: nan - acc: 0.2979 - val_loss: 2.3168 - val_f1: 0.1028 - val_acc: 0.3161\n",
      "Epoch 2/70\n",
      "293159/293159 [==============================] - 18s - loss: 2.2728 - f1: 0.1279 - acc: 0.3245 - val_loss: 2.2752 - val_f1: 0.1261 - val_acc: 0.3243\n",
      "Epoch 3/70\n",
      "293159/293159 [==============================] - 18s - loss: 2.2354 - f1: 0.1417 - acc: 0.3332 - val_loss: 2.2605 - val_f1: 0.1332 - val_acc: 0.3293\n",
      "Epoch 4/70\n",
      "293159/293159 [==============================] - 18s - loss: 2.2072 - f1: 0.1543 - acc: 0.3397 - val_loss: 2.2551 - val_f1: 0.1249 - val_acc: 0.3288\n",
      "Epoch 5/70\n",
      "293159/293159 [==============================] - 18s - loss: 2.1798 - f1: 0.1661 - acc: 0.3466 - val_loss: 2.2539 - val_f1: 0.1716 - val_acc: 0.3310\n",
      "Epoch 6/70\n",
      "293159/293159 [==============================] - 18s - loss: 2.1565 - f1: 0.1758 - acc: 0.3524 - val_loss: 2.2555 - val_f1: 0.1475 - val_acc: 0.3309\n",
      "Epoch 7/70\n",
      "293159/293159 [==============================] - 17s - loss: 2.1319 - f1: 0.1867 - acc: 0.3588 - val_loss: 2.2582 - val_f1: 0.1910 - val_acc: 0.3325\n",
      "Epoch 8/70\n",
      "293159/293159 [==============================] - 18s - loss: 2.1076 - f1: 0.1969 - acc: 0.3644 - val_loss: 2.2747 - val_f1: 0.1746 - val_acc: 0.3281\n",
      "Epoch 9/70\n",
      "293159/293159 [==============================] - 17s - loss: 2.0831 - f1: 0.2069 - acc: 0.3713 - val_loss: 2.2734 - val_f1: 0.1687 - val_acc: 0.3285\n",
      "Epoch 10/70\n",
      "293159/293159 [==============================] - 18s - loss: 2.0573 - f1: 0.2180 - acc: 0.3771 - val_loss: 2.2962 - val_f1: 0.1856 - val_acc: 0.3252\n",
      "Epoch 11/70\n",
      "293159/293159 [==============================] - 18s - loss: 2.0307 - f1: 0.2291 - acc: 0.3842 - val_loss: 2.3118 - val_f1: 0.1669 - val_acc: 0.3211\n",
      "Epoch 12/70\n",
      "293159/293159 [==============================] - 18s - loss: 2.0067 - f1: 0.2405 - acc: 0.3893 - val_loss: 2.3182 - val_f1: 0.1768 - val_acc: 0.3230\n",
      "Epoch 13/70\n",
      "293159/293159 [==============================] - 18s - loss: 1.9799 - f1: 0.2520 - acc: 0.3968 - val_loss: 2.3393 - val_f1: 0.1731 - val_acc: 0.3150\n",
      "Epoch 14/70\n",
      "293159/293159 [==============================] - 18s - loss: 1.9530 - f1: 0.2652 - acc: 0.4034 - val_loss: 2.3710 - val_f1: 0.1955 - val_acc: 0.3152\n",
      "Epoch 15/70\n",
      "293159/293159 [==============================] - 18s - loss: 1.9268 - f1: 0.2760 - acc: 0.4101 - val_loss: 2.3862 - val_f1: 0.2066 - val_acc: 0.3160\n",
      "Epoch 16/70\n",
      "293159/293159 [==============================] - 19s - loss: 1.9004 - f1: 0.2887 - acc: 0.4169 - val_loss: 2.4180 - val_f1: 0.2053 - val_acc: 0.3127\n",
      "Epoch 17/70\n",
      "293159/293159 [==============================] - 19s - loss: 1.8760 - f1: 0.3008 - acc: 0.4243 - val_loss: 2.4399 - val_f1: 0.1895 - val_acc: 0.3054\n",
      "Epoch 18/70\n",
      "293159/293159 [==============================] - 18s - loss: 1.8503 - f1: 0.3110 - acc: 0.4300 - val_loss: 2.4901 - val_f1: 0.2065 - val_acc: 0.3046\n",
      "Epoch 19/70\n",
      "293159/293159 [==============================] - 18s - loss: 1.8268 - f1: 0.3220 - acc: 0.4367 - val_loss: 2.4961 - val_f1: 0.1927 - val_acc: 0.3007\n",
      "Epoch 20/70\n",
      "293159/293159 [==============================] - 20s - loss: 1.8040 - f1: 0.3330 - acc: 0.4431 - val_loss: 2.5269 - val_f1: 0.2013 - val_acc: 0.3019\n",
      "Epoch 21/70\n",
      "293159/293159 [==============================] - 18s - loss: 1.7812 - f1: 0.3444 - acc: 0.4500 - val_loss: 2.5464 - val_f1: 0.2006 - val_acc: 0.2960\n",
      "Epoch 22/70\n",
      "293159/293159 [==============================] - 17s - loss: 1.7602 - f1: 0.3530 - acc: 0.4544 - val_loss: 2.5800 - val_f1: 0.2030 - val_acc: 0.2960\n",
      "Epoch 23/70\n",
      "293159/293159 [==============================] - 17s - loss: 1.7395 - f1: 0.3626 - acc: 0.4605 - val_loss: 2.5972 - val_f1: 0.2020 - val_acc: 0.2933\n",
      "Epoch 24/70\n",
      "293159/293159 [==============================] - 17s - loss: 1.7202 - f1: 0.3706 - acc: 0.4660 - val_loss: 2.6564 - val_f1: 0.2114 - val_acc: 0.2947\n",
      "Epoch 25/70\n",
      "293159/293159 [==============================] - 17s - loss: 1.7017 - f1: 0.3793 - acc: 0.4707 - val_loss: 2.6772 - val_f1: 0.2142 - val_acc: 0.2896\n",
      "Epoch 26/70\n",
      "293159/293159 [==============================] - 17s - loss: 1.6815 - f1: 0.3878 - acc: 0.4762 - val_loss: 2.7135 - val_f1: 0.2207 - val_acc: 0.2914\n",
      "Epoch 27/70\n",
      "293159/293159 [==============================] - 17s - loss: 1.6625 - f1: 0.3972 - acc: 0.4812 - val_loss: 2.7173 - val_f1: 0.2101 - val_acc: 0.2868\n",
      "Epoch 28/70\n",
      "293159/293159 [==============================] - 17s - loss: 1.6469 - f1: 0.4041 - acc: 0.4852 - val_loss: 2.7905 - val_f1: 0.2250 - val_acc: 0.2869\n",
      "Epoch 29/70\n",
      "293159/293159 [==============================] - 17s - loss: 1.6302 - f1: 0.4112 - acc: 0.4900 - val_loss: 2.7834 - val_f1: 0.2110 - val_acc: 0.2811\n",
      "Epoch 30/70\n",
      "293159/293159 [==============================] - 17s - loss: 1.6155 - f1: 0.4168 - acc: 0.4937 - val_loss: 2.8278 - val_f1: 0.2237 - val_acc: 0.2854\n",
      "Epoch 31/70\n",
      "293159/293159 [==============================] - 17s - loss: 1.5993 - f1: 0.4234 - acc: 0.4987 - val_loss: 2.8419 - val_f1: 0.2173 - val_acc: 0.2824\n",
      "Epoch 32/70\n",
      "293159/293159 [==============================] - 17s - loss: 1.5834 - f1: 0.4304 - acc: 0.5033 - val_loss: 2.9076 - val_f1: 0.2246 - val_acc: 0.2819\n",
      "Epoch 33/70\n",
      "293159/293159 [==============================] - 17s - loss: 1.5702 - f1: 0.4364 - acc: 0.5063 - val_loss: 2.9160 - val_f1: 0.2170 - val_acc: 0.2767\n",
      "Epoch 34/70\n",
      "293159/293159 [==============================] - 18s - loss: 1.5576 - f1: 0.4424 - acc: 0.5105 - val_loss: 2.9297 - val_f1: 0.2094 - val_acc: 0.2737\n",
      "Epoch 35/70\n",
      "293159/293159 [==============================] - 17s - loss: 1.5436 - f1: 0.4473 - acc: 0.5135 - val_loss: 2.9816 - val_f1: 0.2119 - val_acc: 0.2729\n",
      "Epoch 36/70\n",
      "293159/293159 [==============================] - 19s - loss: 1.5298 - f1: 0.4547 - acc: 0.5181 - val_loss: 3.0121 - val_f1: 0.2160 - val_acc: 0.2701\n",
      "Epoch 37/70\n",
      "293159/293159 [==============================] - 19s - loss: 1.5176 - f1: 0.4591 - acc: 0.5210 - val_loss: 3.0185 - val_f1: 0.2230 - val_acc: 0.2737\n",
      "Epoch 38/70\n",
      "293159/293159 [==============================] - 17s - loss: 1.5076 - f1: 0.4640 - acc: 0.5249 - val_loss: 3.0277 - val_f1: 0.2162 - val_acc: 0.2735\n",
      "Epoch 39/70\n",
      "293159/293159 [==============================] - 17s - loss: 1.4948 - f1: 0.4683 - acc: 0.5273 - val_loss: 3.0751 - val_f1: 0.2250 - val_acc: 0.2741\n",
      "Epoch 40/70\n",
      "293159/293159 [==============================] - 18s - loss: 1.4849 - f1: 0.4728 - acc: 0.5309 - val_loss: 3.1431 - val_f1: 0.2268 - val_acc: 0.2733\n",
      "Epoch 41/70\n",
      "293159/293159 [==============================] - 18s - loss: 1.4721 - f1: 0.4786 - acc: 0.5335 - val_loss: 3.1614 - val_f1: 0.2157 - val_acc: 0.2675\n",
      "Epoch 42/70\n",
      "293159/293159 [==============================] - 17s - loss: 1.4625 - f1: 0.4831 - acc: 0.5371 - val_loss: 3.1437 - val_f1: 0.2193 - val_acc: 0.2688\n",
      "Epoch 43/70\n",
      "293159/293159 [==============================] - 17s - loss: 1.4532 - f1: 0.4859 - acc: 0.5397 - val_loss: 3.1864 - val_f1: 0.2196 - val_acc: 0.2669\n",
      "Epoch 44/70\n",
      "293159/293159 [==============================] - 17s - loss: 1.4434 - f1: 0.4906 - acc: 0.5426 - val_loss: 3.1884 - val_f1: 0.2209 - val_acc: 0.2679\n",
      "Epoch 45/70\n",
      "293159/293159 [==============================] - 18s - loss: 1.4355 - f1: 0.4930 - acc: 0.5434 - val_loss: 3.2483 - val_f1: 0.2193 - val_acc: 0.2641\n",
      "Epoch 46/70\n",
      "293159/293159 [==============================] - 17s - loss: 1.4246 - f1: 0.4984 - acc: 0.5473 - val_loss: 3.2873 - val_f1: 0.2191 - val_acc: 0.2622\n",
      "Epoch 47/70\n",
      "293159/293159 [==============================] - 17s - loss: 1.4142 - f1: 0.5015 - acc: 0.5506 - val_loss: 3.3086 - val_f1: 0.2186 - val_acc: 0.2618\n",
      "Epoch 48/70\n",
      "293159/293159 [==============================] - 19s - loss: 1.4061 - f1: 0.5058 - acc: 0.5532 - val_loss: 3.3477 - val_f1: 0.2226 - val_acc: 0.2639\n",
      "Epoch 49/70\n",
      "293159/293159 [==============================] - 19s - loss: 1.3971 - f1: 0.5090 - acc: 0.5544 - val_loss: 3.3638 - val_f1: 0.2137 - val_acc: 0.2581\n",
      "Epoch 50/70\n",
      "293159/293159 [==============================] - 17s - loss: 1.3897 - f1: 0.5123 - acc: 0.5570 - val_loss: 3.4010 - val_f1: 0.2203 - val_acc: 0.2601\n",
      "Epoch 51/70\n",
      "293159/293159 [==============================] - 17s - loss: 1.3816 - f1: 0.5153 - acc: 0.5601 - val_loss: 3.4130 - val_f1: 0.2235 - val_acc: 0.2632\n",
      "Epoch 52/70\n",
      "293159/293159 [==============================] - 19s - loss: 1.3732 - f1: 0.5190 - acc: 0.5625 - val_loss: 3.4505 - val_f1: 0.2232 - val_acc: 0.2628\n",
      "Epoch 53/70\n",
      "293159/293159 [==============================] - 18s - loss: 1.3643 - f1: 0.5229 - acc: 0.5653 - val_loss: 3.4083 - val_f1: 0.2146 - val_acc: 0.2567\n",
      "Epoch 54/70\n",
      "293159/293159 [==============================] - 17s - loss: 1.3584 - f1: 0.5246 - acc: 0.5663 - val_loss: 3.4977 - val_f1: 0.2242 - val_acc: 0.2615\n",
      "Epoch 55/70\n",
      "293159/293159 [==============================] - 19s - loss: 1.3517 - f1: 0.5269 - acc: 0.5684 - val_loss: 3.4737 - val_f1: 0.2163 - val_acc: 0.2555\n",
      "Epoch 56/70\n",
      "293159/293159 [==============================] - 19s - loss: 1.3419 - f1: 0.5316 - acc: 0.5712 - val_loss: 3.5190 - val_f1: 0.2266 - val_acc: 0.2603\n",
      "Epoch 57/70\n",
      "293159/293159 [==============================] - 21s - loss: 1.3354 - f1: 0.5347 - acc: 0.5735 - val_loss: 3.5112 - val_f1: 0.2231 - val_acc: 0.2602\n",
      "Epoch 58/70\n",
      "293159/293159 [==============================] - 19s - loss: 1.3281 - f1: 0.5375 - acc: 0.5756 - val_loss: 3.5848 - val_f1: 0.2196 - val_acc: 0.2560\n",
      "Epoch 59/70\n",
      "293159/293159 [==============================] - 17s - loss: 1.3230 - f1: 0.5396 - acc: 0.5775 - val_loss: 3.5785 - val_f1: 0.2117 - val_acc: 0.2523\n",
      "Epoch 60/70\n",
      "293159/293159 [==============================] - 17s - loss: 1.3132 - f1: 0.5436 - acc: 0.5794 - val_loss: 3.5995 - val_f1: 0.2219 - val_acc: 0.2578\n",
      "Epoch 61/70\n",
      "293159/293159 [==============================] - 20s - loss: 1.3091 - f1: 0.5449 - acc: 0.5802 - val_loss: 3.6347 - val_f1: 0.2253 - val_acc: 0.2590\n",
      "Epoch 62/70\n",
      "293159/293159 [==============================] - 21s - loss: 1.3011 - f1: 0.5485 - acc: 0.5830 - val_loss: 3.6778 - val_f1: 0.2300 - val_acc: 0.2610\n",
      "Epoch 63/70\n",
      "293159/293159 [==============================] - 18s - loss: 1.2949 - f1: 0.5511 - acc: 0.5850 - val_loss: 3.7191 - val_f1: 0.2258 - val_acc: 0.2565\n",
      "Epoch 64/70\n",
      "293159/293159 [==============================] - 17s - loss: 1.2913 - f1: 0.5523 - acc: 0.5861 - val_loss: 3.6824 - val_f1: 0.2214 - val_acc: 0.2549\n",
      "Epoch 65/70\n",
      "293159/293159 [==============================] - 17s - loss: 1.2830 - f1: 0.5549 - acc: 0.5889 - val_loss: 3.6953 - val_f1: 0.2213 - val_acc: 0.2538\n",
      "Epoch 66/70\n",
      "293159/293159 [==============================] - 17s - loss: 1.2778 - f1: 0.5573 - acc: 0.5898 - val_loss: 3.7111 - val_f1: 0.2092 - val_acc: 0.2455\n",
      "Epoch 67/70\n",
      "293159/293159 [==============================] - 17s - loss: 1.2708 - f1: 0.5605 - acc: 0.5922 - val_loss: 3.7671 - val_f1: 0.2188 - val_acc: 0.2524\n",
      "Epoch 68/70\n",
      "293159/293159 [==============================] - 17s - loss: 1.2670 - f1: 0.5618 - acc: 0.5926 - val_loss: 3.7501 - val_f1: 0.2174 - val_acc: 0.2505\n",
      "Epoch 69/70\n",
      "293159/293159 [==============================] - 18s - loss: 1.2619 - f1: 0.5641 - acc: 0.5949 - val_loss: 3.7813 - val_f1: 0.2239 - val_acc: 0.2548\n",
      "Epoch 70/70\n",
      "293159/293159 [==============================] - 17s - loss: 1.2541 - f1: 0.5669 - acc: 0.5963 - val_loss: 3.8275 - val_f1: 0.2123 - val_acc: 0.2463\n"
     ]
    }
   ],
   "source": [
    "model=createmodel()\n",
    "history = History()\n",
    "res = model.fit(embeddings_train, y_train, batch_size=512, epochs=70, validation_data=(embeddings_val, y_val),\n",
    "                callbacks=[history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.save('nn_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "modl= load_model('nn_model.h5',custom_objects={'f1': f1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history=res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['acc', 'f1', 'loss', 'val_acc', 'val_f1', 'val_loss']\n"
     ]
    }
   ],
   "source": [
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['f1'])\n",
    "plt.plot(history.history['val_f1'])\n",
    "plt.title('model f1 score')\n",
    "plt.ylabel('f1')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
